# -*- coding: utf-8 -*-
"""Aref.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wtRfpjrgnS6jLIjDTNjuJFJh4_NVdNum

# Llama from scratch

I want to provide some tips from my experience implementing a paper. I'm going to cover my tips so far from implementing a dramatically scaled-down version of [Llama](https://arxiv.org/pdf/2302.13971.pdf) for training [TinyShakespeare](https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt). This post is heavily inspired by Karpathy's [Makemore series](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ), which I highly recommend.

I'm only going to *loosely* follow the layout of their paper; while the formatting and order of sections makes sense for publication, we're going to be implementing the paper. I'll also be skipping over some of the more obvious steps, like setting up a virtual environment and installing dependencies.
"""

#https://github.com/bkitano/llama-from-scratch/blob/main/input.txt
!pip install datasets

import torch
from torch import nn
from torch.nn import functional as F
import numpy as np
from matplotlib import pyplot as plt
import time
import pandas as pd
import json
import zipfile
import os
import gc
gc.enable()

#from datasets import load_dataset
#dataset = load_dataset('umarigan/turkish_wikipedia', split = 'train')

from google.colab import drive
#drive.mount('/content/drive')

## kaggle
!mkdir ~/.kaggle
!touch ~/.kaggle/kaggle.json

### get your kaggle token from kaggle.com
api_token = {"username":"umar47","key":"343cfb0f6675ba93478949757ba1599c"}


with open('/root/.kaggle/kaggle.json', 'w') as file:
    json.dump(api_token, file)

! chmod 600 ~/.kaggle/kaggle.json
! kaggle datasets download redrussianarmy/translated-books-in-turkish #250MB
! kaggle datasets download redrussianarmy/turkish-corpus #750MB

# Extract the face images dataset
dataset_zip1 = "/content/translated-books-in-turkish.zip"
dataset_zip2 = "/content/turkish-corpus.zip"
dataset_dir = "/content/books"

with zipfile.ZipFile(dataset_zip1, "r") as zip_ref:
    zip_ref.extractall("/content/books")

with zipfile.ZipFile(dataset_zip2, "r") as zip_ref:
    zip_ref.extractall("/content/books")

from datasets import load_dataset
# Load the dataset
dataset = load_dataset("umarigan/turkish_corpus_small", split="train[:500000]")  # Replace "your_dataset_name" with the actual dataset name
#dataset = load_dataset("umarigan/turkish_corpus", split="train")  # Replace "your_dataset_name" with the actual dataset name
#
# List of columns to drop
columns_to_drop = ["id"]  # Replace with the actual column names you want to drop

# Remove specified columns
dataset = dataset.remove_columns(columns_to_drop)

# Specify the path to save the text file
output_file_path = "dataset.txt"

# Open the file in write mode
with open(output_file_path, "w", encoding="utf-8") as file:
    # Iterate through the dataset and write each example to the file
    for example in dataset:
        text = example["text"]  # Replace "text" with the actual column name containing the text
        file.write(text + "\n")

del dataset
del text
gc.collect()

file1_path = '/content/books/book_dataset.txt'
file2_path = '/content/books/tr_corpus.txt'
file3_path = '/content/dataset.txt'
output_path = 'combined.txt'

with open(file1_path, 'r') as file1, open(file2_path, 'r') as file2 , open(file3_path, 'r') as file3:
    content1 = file1.read()
    content2 = file2.read()
    content3 = file3.read()

combined_content = content1 + content2 + content3

with open(output_path, 'w') as output_file:
    output_file.write(combined_content)

del content1
del content2
del content3
del combined_content
gc.collect()

file_path = '/content/combined.txt'
lines = open(file_path, 'r').read()
file_size = os.path.getsize(file_path)
vocab = sorted(list(set(lines)))
print(f"The size of {file_path} is {file_size} bytes.  and vocab is {len(vocab)}")
itos = {i:ch for i, ch in enumerate(vocab)}
stoi = {ch:i for i, ch in enumerate(vocab)}
print(lines[150:250])

"""They use the [SentencePiece](https://github.com/google/sentencepiece) byte-pair encoding tokenizer, but we're going to just use a simple character-level tokenizer."""

#MASTER_CONFIG.update({ 'epochs': 10000,})
MASTER_CONFIG = {'vocab_size': len(vocab),
 'batch_size': 32,
 'context_window': 16,
 'd_model': 256,
 'epochs': 5000,
 'log_interval': 10,
 'n_heads': 16,
 'n_layers': 8}

import sentencepiece as spm
import os
"""
#https://discuss.huggingface.co/t/sentencepiece-user-defined-symbols-and-fast-tokenizers/52208
# Train SentencePiece model
file_path = '/content/combined.txt'
output_model_path = '/content/tokenizer.model'
spm.SentencePieceTrainer.train(input=file_path, model_prefix=output_model_path)

# Load trained SentencePiece model
sp = spm.SentencePieceProcessor()
sp.load(output_model_path + ".model")

# Encode and decode functions using SentencePiece
def encode(s):
    return sp.encode_as_ids(s)

def decode(l):
    return sp.decode_ids(l)

# Test the SentencePiece encoding
encoded_text = encode("nas覺ls覺n")
print("Encoded:", encoded_text)

# Test the SentencePiece decoding
decoded_text = decode(encoded_text)
print("Decoded:", decoded_text)
"""

# simple tokenization by characters
from transformers import LlamaTokenizer
model_id = "Trendyol/Trendyol-LLM-7b-chat-v0.1"
#tokenizer = LlamaTokenizer.from_pretrained(model_id)

def encode(s):
    return [stoi[ch] for ch in s]#tokenizer.encode(s)

def decode(l):
    return ''.join([itos[i] for i in l])#tokenizer.decode(l)

print('vocab size:', len(vocab))
encode("nas覺ls覺n")

"""Since our dataset is small enough, we don't need to worry about how we store it in memory etc.

First tip: I'm creating a `config` object that stores some basic model params. It makes our code way more readable and removes constants and magic numbers from the code. I'm not going to use types, as  I want to keep things flexible for now and be able to add more parameters later on.
"""

#decoded_str = decode(encode(lines))

dataset = torch.tensor(encode(lines), dtype=torch.int32)

#dataset = torch.tensor(encode(lines), dtype=torch.int32)
# Define chunk size
chunk_size = 10000

# Chunk the lines
chunks = [lines[i:i+chunk_size] for i in range(0, len(lines), chunk_size)]
del lines
gc.collect()
# Initialize an empty list to store individual tensors
tensor_chunks = []

# Iterate through the chunks
for chunk in chunks:
    tensor_chunk = torch.tensor(encode(chunk), dtype=torch.int32)
    tensor_chunks.append(tensor_chunk)

# Concatenate the tensor chunks along the first dimension
dataset = torch.cat(tensor_chunks)

# Display the shape of the final dataset tensor
print("Final Dataset Shape:", dataset.shape)

len(dataset)#tensor([    1,  6290, 11498,  ..., 33624, 32026,    13], dtype=torch.int32), #370108936

del chunks
del tensor_chunks
gc.collect()

"""Let's create a method to generate our training data and labels for batches. We'll use the same method for validation and test data. Note that I like to test my functions in the same block that I define them, just to make sure they work as expected before moving on."""

#xs[0]#tensor([29882,   790,   295,   314, 32914, 39852, 34596, 32296, 40930, 10288, 747, 29915,   262, 29871, 32927, 29874])
#ys[0]#tensor([  790,   295,   314, 32914, 39852, 34596, 32296, 40930, 10288,   747,29915,   262, 29871, 32927, 29874, 33188])

@torch.compile
def get_batches(data, split, batch_size, context_window, config=MASTER_CONFIG):
    train = data[:int(.8 * len(data))]
    val = data[int(.8 * len(data)): int(.9 * len(data))]
    test = data[int(.9 * len(data)):]

    batch_data = train
    if split == 'val':
        batch_data = val

    if split == 'test':
        batch_data = test

    # pick random starting points
    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))
    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()
    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()
    return x, y

xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])

[(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))]

xs[0]#tensor([29882,   790,   295,   314, 32914, 39852, 34596, 32296, 40930, 10288, 747, 29915,   262, 29871, 32927, 29874])
ys[0]#tensor([  790,   295,   314, 32914, 39852, 34596, 32296, 40930, 10288,   747,29915,   262, 29871, 32927, 29874, 33188])

"""What's interesting about implementing papers is that there are two aspects to the model *working*: compilation (do your tensors all match up from layer to layer), and training (does the loss go down). Figuring out how to ensure that each of your compoenents is working is key to developing your model in a predictable, engineering-minded way.

That's why we're also going to define the method for how we're going to evaluate the model. We want to do this before we even define the model, because we want to be able to use it to evaluate the model as we're training it.
"""

@torch.no_grad()  # don't compute gradients for this function
def evaluate_loss(model, config=MASTER_CONFIG):
    out = {}
    model.eval()
    for split in ["train", "val"]:
        losses = []
        for _ in range(10):
            xb, yb = get_batches(dataset, split, config['batch_size'], config['context_window'])
            _, loss = model(xb, yb)
            losses.append(loss.item())
        out[split] = np.mean(losses)
    model.train()
    return out

"""## Setting up a working base model

Here's a basic feed-forward neural network with embeddings. It's the base model we're going to start with, and then swap out parts of it as we go along until we eventually end up with the model as described in Llama.
"""

def train(model, optimizer, scheduler=None, config=MASTER_CONFIG, print_logs=False):
    losses = []
    start_time = time.time()
    for epoch in range(config['epochs']):
        optimizer.zero_grad()

        xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])

        logits, loss = model(xs, targets=ys)
        loss.backward()
        optimizer.step()
        print(f"loss at epoch {epoch} is {loss}")
        if scheduler:
            scheduler.step()

        if epoch % config['log_interval'] == 0:
            batch_time = time.time() - start_time
            x = evaluate_loss(model)
            losses += [x]
            if print_logs:
                print(f"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config['epochs'] - epoch)/config['log_interval'] :.3f}")
            start_time = time.time()

            if scheduler:
                print("lr: ", scheduler.get_lr())

    print("validation loss: ", losses[-1]['val'])
    return pd.DataFrame(losses).plot()

def generate(model, config=MASTER_CONFIG, max_new_tokens=30):
    idx = torch.zeros(5, 1).long()
    for _ in range(max_new_tokens):
        # call the model
        logits = model(idx[:, -config['context_window']:])
        last_time_step_logits = logits[
            :, -1, :
        ]  # all the batches (1), last time step, all the logits
        p = F.softmax(last_time_step_logits, dim=-1)  # softmax to get probabilities
        idx_next = torch.multinomial(
            p, num_samples=1
        )  # sample from the distribution to get the next token
        idx = torch.cat([idx, idx_next], dim=-1)  # append to the sequence
    return [decode(x) for x in idx.tolist()]

#generate(model)

"""It's not half bad, but also not half good. But now we have a working model that is training to a validation loss. So here we'll iterate on our model to make it closer to Llama.

## Llama specifics
Llama describes three architectural modifications to the original Transformer:
1. RMSNorm for pre-normalization
1. Rotary embeddings
1. SwiGLU activation function

We're going to add each one, one at a time to our base model, and iterate.

### RMSNorm
"""

@torch.compile
class RMSNorm(nn.Module):
    def __init__(self, layer_shape, eps=1e-8, bias=False):
        super(RMSNorm, self).__init__()
        self.register_parameter("scale", nn.Parameter(torch.ones(layer_shape)))

    def forward(self, x):
        """
        assumes shape is (batch, seq_len, d_model)
        """
        # frob norm is not the same as RMS. RMS = 1/sqrt(N) * frob norm
        ff_rms = torch.linalg.norm(x, dim=(1,2)) * x[0].numel() ** -.5
        raw = x / ff_rms.unsqueeze(-1).unsqueeze(-1)
        return self.scale[:x.shape[1], :].unsqueeze(0) * raw

"""We want to test to ensure that the RMSNorm is doing what we think it should. We can do this the old-fashioned way: row-wise comparisons. The RMSNorm has the property where the norm of the layer will be the square root of the number of elements in the layer, so we can check that for every layer.

### Rotary Embeddings
[RoPE](https://arxiv.org/pdf/2104.09864.pdf) is a kind of positional encoding for transformers. In Attention is All You Need, the authors propose two kinds of positional encodings, learned and fixed. In RoPE, the authors propose embedding the position of a token in a sequence by rotating the embedding, with a different rotation at each position.

#### RoPEAttentionHead (uses get_rotary_matrix)
"""

@torch.compile
class RoPEAttentionHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.w_q = nn.Linear(config['d_model'], config['d_model'], bias=False)
        self.w_k = nn.Linear(config['d_model'], config['d_model'], bias=False)
        self.w_v = nn.Linear(config['d_model'], config['d_model'], bias=False)

        # Call the static method using the class name
        self.R = RoPEAttentionHead.get_rotary_matrix(config['context_window'], config['d_model'])

    @staticmethod
    def get_rotary_matrix(context_window, embedding_dim):
        R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)
        for position in range(context_window):
            for i in range(embedding_dim//2):
                theta = 10000. ** (-2.*(i - 1) / embedding_dim)
                m_theta = position * theta
                R[position, 2*i, 2*i] = np.cos(m_theta)
                R[position, 2*i, 2*i+1] = -np.sin(m_theta)
                R[position, 2*i+1, 2*i] = np.sin(m_theta)
                R[position, 2*i+1, 2*i+1] = np.cos(m_theta)
        return R

    def forward(self, x, return_attn_weights=False):
        b, m, d = x.shape

        q = self.w_q(x)
        k = self.w_k(x)
        v = self.w_v(x)


        q_rotated = (torch.bmm(q.transpose(0, 1), self.R[:m])).transpose(0, 1)
        k_rotated = (torch.bmm(k.transpose(0, 1), self.R[:m])).transpose(0, 1)

        activations = F.scaled_dot_product_attention(
            q_rotated, k_rotated, v, dropout_p=.1
        )

        if return_attn_weights:
            attn_weights = torch.bmm(q_rotated, k_rotated.transpose(1, 2)) / np.sqrt(d)
            attn_weights = F.softmax(attn_weights, dim=-1)
            return activations, attn_weights
        return activations

# Create an instance of RoPEAttentionHead
#layer = RoPEAttentionHead(config)

# Test with a random batch
#batch = torch.randn((config['batch_size'], config['context_window'], config['d_model']))
#output, attn_weights = layer(batch, return_attn_weights=True)

"""> Tip here: know the difference between tensor dimensions at train time vs tensor dimensions at inference time.

Although at train time, you can expect your tensor dimensions to match your model parameters closely, eg `batch.shape = (config['batch_size'], config['context_window'], config['d_model'])`, at inference time, you may have to deal with a single example, eg `batch.shape = (1, 1, config['d_model'])`. For this reason, you need to make sure that when you're indexing in the `forward` pass, you're indexing using shapes derived from the input, not necessarily the model parameters.

Let's make sure it does what we think it does. For this layer, we're going to want to test three things:
1. that it rotates embeddings the way we think it does
2. that the attention mask used for causal attention is working properly.

#### RoPEMultiheadAttention (uses RoPEAttentionHead)
"""

# definitely there's an optimization we could make where we cache the rotation matrices, but skip.
@torch.compile
class RoPEMultiheadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.heads = nn.ModuleList([
            RoPEAttentionHead(config) for _ in range(config['n_heads'])
        ])
        self.linear = nn.Linear(config['n_heads'] * config['d_model'], config['d_model'])
        self.dropout = nn.Dropout(.1)

    def forward(self, x):
        heads = [h(x) for h in self.heads]
        x = torch.cat(heads, dim=-1)
        x = self.linear(x)
        x = self.dropout(x)
        return x

"""So it looks terrible. What is happening here? Let's start debugging this by looking at the attention.

#### RoPEMaskedAttentionHead (uses RoPEMaskedAttentionHead)
"""

@torch.compile
class RoPEMaskedAttentionHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.w_q = nn.Linear(config['d_model'], config['d_model'], bias=False)
        self.w_k = nn.Linear(config['d_model'], config['d_model'], bias=False)
        self.w_v = nn.Linear(config['d_model'], config['d_model'], bias=False)

        # Call the static method using the class name
        self.R = RoPEMaskedAttentionHead.get_rotary_matrix(config['context_window'], config['d_model'])

    @staticmethod
    def get_rotary_matrix(context_window, embedding_dim):
        R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)
        for position in range(context_window):
            for i in range(embedding_dim//2):
                theta = 10000. ** (-2.*(i - 1) / embedding_dim)
                m_theta = position * theta
                R[position, 2*i, 2*i] = np.cos(m_theta)
                R[position, 2*i, 2*i+1] = -np.sin(m_theta)
                R[position, 2*i+1, 2*i] = np.sin(m_theta)
                R[position, 2*i+1, 2*i+1] = np.cos(m_theta)
        return R

    def forward(self, x, return_attn_weights=False):
        b, m, d = x.shape

        q = self.w_q(x)
        k = self.w_k(x)
        v = self.w_v(x)



        q_rotated = (torch.bmm(q.transpose(0, 1), self.R[:m])).transpose(0, 1)
        k_rotated = (torch.bmm(k.transpose(0, 1), self.R[:m])).transpose(0, 1)


        activations = F.scaled_dot_product_attention(
            q_rotated, k_rotated, v, dropout_p=.1, is_causal=True
        )

        if return_attn_weights:
            attn_mask = torch.tril(torch.ones((m, m)), diagonal=0)
            attn_weights = torch.bmm(q_rotated, k_rotated.transpose(1, 2)) / np.sqrt(d) + attn_mask
            attn_weights = F.softmax(attn_weights, dim=-1)
            return activations, attn_weights
        return activations

# Create an instance of RoPEMaskedAttentionHead
config = {
    'batch_size': 32,
    'd_model': 512,
    'n_heads': 8,
    'context_window': 128,
}


#layer = RoPEMaskedAttentionHead(config)

# Test with a random batch
#batch = torch.randn((config['batch_size'], config['context_window'], config['d_model']))
#output, attn_weights = layer(batch, return_attn_weights=True)

#plt.imshow(attn_weights[0].detach().numpy())
#plt.colorbar()

"""#### RoPEMaskedMultiheadAttention (uses RoPEMaskedAttentionHead)"""

# definitely there's an optimization we could make where we cache the rotation matrices, but skip.
@torch.compile
class RoPEMaskedMultiheadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.heads = nn.ModuleList([
            RoPEMaskedAttentionHead(config) for _ in range(config['n_heads'])
        ])
        self.linear = nn.Linear(config['n_heads'] * config['d_model'], config['d_model'])
        self.dropout = nn.Dropout(.1)

    def forward(self, x):
        heads = [h(x) for h in self.heads]
        x = torch.cat(heads, dim=-1)
        x = self.linear(x)
        x = self.dropout(x)
        return x

"""Much better, our loss is now not merely dropping to near-zero. It looks like we can drive our loss down even lower. Let's do that by updating master config.

### SwiGLU

As it says in the paper, "We replace the ReLU non-linearity by the SwiGLU activation function...we use a dimension of $\frac{2}{3} 4d$ isntead of $4d$ as in PaLM." SwiGLU is defined as:
$$
\text{SwiGLU}(x) = \text{Swish}_\beta (xW + b) \otimes (xV + c)
$$
where $\otimes$ is a component-wise product. The Swish function is defined as:
$$
\text{Swish}_\beta(x) = x \sigma(\beta x)
$$
where $\beta$ is a learnable parameter.
"""

@torch.compile
class SwiGLU(nn.Module):
    """
    Swish-Gated Linear Unit
    https://arxiv.org/pdf/2002.05202v1.pdf
    """
    def __init__(self, size):
        super().__init__()
        self.config = config
        self.linear_gate = nn.Linear(size, size)
        self.linear = nn.Linear(size, size)
        self.beta = torch.randn(1, requires_grad=True)

        self.beta = nn.Parameter(torch.ones(1))
        self.register_parameter("beta", self.beta)

    def forward(self, x):
        swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))
        out = swish_gate * self.linear(x)
        return out

"""Now, let's add multiple layers of RopeAttention by creating blocks.

#### llama block(uses RMSNorm, RoPEMaskedMultiheadAttention and SwiGLU)
"""

# add RMSNorm and residual conncection
@torch.compile
class LlamaBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        self.rms = RMSNorm((config['context_window'], config['d_model']))

        self.attention = RoPEMaskedMultiheadAttention(config)
        self.feedforward = nn.Sequential(
            nn.Linear(config['d_model'], config['d_model']),
            SwiGLU(config['d_model']),
        )

    def forward(self, x):
        x = self.rms(x) # rms pre-normalization
        x = x + self.attention(x)

        x = self.rms(x) # rms pre-normalization
        x = x + self.feedforward(x)
        return x

block = LlamaBlock(MASTER_CONFIG)
block(torch.randn(MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'], MASTER_CONFIG['d_model']));

"""#### llama(uses llama block and SwiGLU)"""

from collections import OrderedDict
@torch.compile
class Llama(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.embeddings = nn.Embedding(config['vocab_size'], config['d_model'])
        self.llama_blocks = nn.Sequential(
            OrderedDict([(f"llama_{i}", LlamaBlock(config)) for i in range(config['n_layers'])])
        )

        self.ffn = nn.Sequential(
            nn.Linear(config['d_model'], config['d_model']),
            SwiGLU(config['d_model']),
            nn.Linear(config['d_model'], config['vocab_size']),
        )
        print("model params:", sum([m.numel() for m in self.parameters()]))

    def forward(self, idx, targets=None):
        x = self.embeddings(idx)
        #print("Input indices:", idx)
        #print("Embedding layer num_embeddings:", self.embeddings.num_embeddings)
        x = self.llama_blocks(x)
        logits = self.ffn(x)

        if targets is None:
            return logits

        else:
            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))
            return logits, loss

llama = Llama(MASTER_CONFIG)
optimizer = torch.optim.Adam(llama.parameters())
#train(llama, optimizer, config=MASTER_CONFIG)

"""It looks like we can drive the loss down even more, and although we're overfitting a little, I think we can still do better. Let's train longer."""

MASTER_CONFIG.update({
    'epochs': 20000,
})
train(llama, optimizer, scheduler=None, config=MASTER_CONFIG)#1.1885965824127198, 1h

"""It seems we can go even lower, still without serious overfitting. Either there is a leak, or it's actually doing well. The loss here is 1.08, which is equivalent to choosing between 2.9 tokens randomly."""

MASTER_CONFIG_ = {'vocab_size': 12765,
 'batch_size': 32,
 'context_window': 16,
 'd_model': 128,
 'epochs': 1000,
 'log_interval': 10,
 'n_heads': 8,
 'n_layers': 4}

#train(llama, optimizer, config=MASTER_CONFIG)

generate(llama, MASTER_CONFIG, 500)[0]

llama.parameters

"""At this point, we've hit the bottom with our training. Let's test on the test set."""

xs, ys = get_batches(dataset, 'test', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])

logits, loss = llama(xs, ys)

print(loss)

"""# Miscellaneous

## Check for Gradient Flows

Let's inspect the gradients, we want to see how they're flowing. If there are too many gradients where the value is close to 0, that's a problem.
"""

MASTER_CONFIG

# Generate a sequence of tokens
generated_sequence = generate(llama, config=MASTER_CONFIG, max_new_tokens=30)
generated_sequence

llama.eval()
@torch.compile
def generate_next_token(model,idx, config=MASTER_CONFIG, max_new_tokens=30):
    #idx = torch.zeros(5, 1).long()
    for _ in range(max_new_tokens):
        # call the model
        logits = model(idx[:, -config['context_window']:])
        last_time_step_logits = logits[
            :, -1, :
        ]  # all the batches (1), last time step, all the logits
        p = F.softmax(last_time_step_logits, dim=-1)  # softmax to get probabilities
        idx_next = torch.multinomial(
            p, num_samples=1
        )  # sample from the distribution to get the next token
        idx = torch.cat([idx, idx_next], dim=-1)  # append to the sequence
    return [decode(x) for x in idx.tolist()]

codes = [encode("araban覺n tekerlekleri ")]
idx = torch.tensor(codes).long()
res = generate_next_token(llama, idx, config=MASTER_CONFIG, max_new_tokens=30)
res

# print the percentage that are near 0
def show_grads(model, tol=1e-2):
    return sorted([(name, 100.0 * float(torch.sum(torch.abs(param) <= tol)) / float(param.nelement())) for name, param in model.named_parameters() if param.requires_grad], key=lambda t: t[1], reverse=True)

#show_grads(llama)

"""Here, for all of our parameter gradients, the vast majority are non-zero, which is great. If we start to see this number peak higher, then our gradients would not be flowing.

## Experiment with hyperparams, aka "change the oven settings"

In the original Llama paper, the authors use Cosine Annealing learning schedule. We didn't do that here, because I experimented and saw that it was worse.
"""

MASTER_CONFIG.update({
    "epochs": 1000
})
llama_with_cosine = Llama(MASTER_CONFIG)
llama_optimizer = torch.optim.Adam(
    llama.parameters(),
    betas=(.9, .95),
    weight_decay=.1,
    eps=1e-9,
    lr=1e-3
)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(llama_optimizer, 300, eta_min=1e-5)
#train(llama_with_cosine, llama_optimizer, scheduler=scheduler)

#show_grads(llama_with_cosine, 1e-5)

# Save the entire model
torch.save(llama, 'llama_model.pth')

# If you want to save only the model parameters
torch.save(llama.state_dict(), 'llama_model_params.pth')

"""Even at an extremely low tolerance, the attention biases are not getting any signal. I'm not sure why the learning schedule from the paper doesn't work, but the lesson here is simple: start simple.

save model:
https://levelup.gitconnected.com/building-a-million-parameter-llm-from-scratch-using-python-f612398f06c2
"""

!pip install createllm
#https://pythonscholar.com/build-a-large-language-model-from-scratch/

from createllm import CreateLLM

path = "/content/dataset.txt"

#model = CreateLLM.GPTTrainer(path,max_iters=100)
#model = model.trainer()